1. `app.py`ï¼šå®Œæ•´ Streamlit Appï¼ˆå« AI Note Keeperã€æ–°å¢åŠŸèƒ½ã€31 Agents UI æ”¯æ´ã€å¤šå®¶æ¨¡å‹é¸æ“‡ã€API Key ç®¡ç†ã€WOW UI æ¶æ§‹ï¼‰ã€‚  
2. `agents.yaml`ï¼šé€²éšç‰ˆï¼ŒåŒ…å« 31 å€‹ä¸­æ–‡ä»£ç†äººè¨­å®šã€‚

ä½ å¯ä»¥ç›´æ¥æŠŠé€™å…©å€‹æª”æ¡ˆæ”¾åˆ° Hugging Face Space å°ˆæ¡ˆè£¡åŸ·è¡Œï¼ˆè¨˜å¾—åœ¨ `requirements.txt` åŠ ä¸Š `streamlit`, `pyyaml`, ä»¥åŠå„å®¶ LLM SDK/HTTP clientï¼‰ã€‚

---

## 1. `app.py`

```python
import os
import random
from typing import Dict, Any, List

import streamlit as st
import yaml

# =========================
# åŸºæœ¬è¨­å®š
# =========================

st.set_page_config(
    page_title="BioChain Analyst Pro v2.0",
    layout="wide",
)

# =========================
# å¤šèªç³» (ç°¡åŒ–ç‰ˆï¼Œé è¨­ç¹é«”ä¸­æ–‡)
# =========================

TRANSLATIONS = {
    "zh-TW": {
        "title": "é†«ç™‚å™¨æä¾›æ‡‰éˆè¿½è¹¤ç³»çµ± v2.0 - BioChain Analyst Pro",
        "dashboard": "å„€è¡¨æ¿",
        "data_studio": "è³‡æ–™å·¥ä½œå®¤",
        "network_graph": "ä¾›æ‡‰éˆç¶²è·¯åœ–",
        "time_trends": "æ™‚é–“è¶¨å‹¢",
        "ai_agents": "AI ä»£ç†äººæ§åˆ¶å°",
        "ai_note_keeper": "AI ç­†è¨˜ç®¡å®¶",
        "system_logs": "ç³»çµ±èˆ‡ç´€éŒ„",
        "theme_selector": "èŠ±å‰ä¸»é¡Œé¸æ“‡å™¨ (Jackslot)",
        "spin_random_theme": "éš¨æ©ŸæŠ½ä¸»é¡Œ",
        "apply_theme": "å¥—ç”¨ä¸»é¡Œ",
        "select_theme": "é¸æ“‡ä¸»é¡Œ",
        "api_keys": "API é‡‘é‘°ç®¡ç†",
        "dataset": "è³‡æ–™é›†",
        "upload_csv": "ä¸Šå‚³ CSV",
        "or_use_mock": "æˆ–ä½¿ç”¨å…§å»ºç¤ºç¯„è³‡æ–™",
        "ai_agents_pipeline": "å¤šä»£ç†äºº AI åˆ†ææµç¨‹",
        "execution_mode": "åŸ·è¡Œæ¨¡å¼",
        "run_all_enabled": "ä¾åºåŸ·è¡Œæ‰€æœ‰å·²å•Ÿç”¨ä»£ç†äºº",
        "run_single": "é€ä¸€åŸ·è¡Œä»£ç†äºº",
        "enable_agent": "å•Ÿç”¨æ­¤ä»£ç†äºº",
        "prompt_template": "Prompt æ¨¡æ¿",
        "max_tokens": "Max Tokens",
        "use_prev_output_as_input": "å°‡ä¸Šä¸€å€‹ä»£ç†äººçš„è¼¸å‡ºä½œç‚ºè¼¸å…¥ï¼ˆå¯å†ç·¨è¼¯ï¼‰",
        "agent_input": "æ­¤ä»£ç†äººçš„è¼¸å…¥å…§å®¹ï¼ˆå¯æ‰‹å‹•ç·¨è¼¯ï¼‰",
        "run_agent": "åŸ·è¡Œæ­¤ä»£ç†äºº",
        "view_mode": "è¼¸å‡ºæª¢è¦–æ¨¡å¼",
        "plain_text": "ç´”æ–‡å­—",
        "markdown": "Markdown æ¸²æŸ“",
        "agent_output": "ä»£ç†äººè¼¸å‡ºçµæœ",
        "you_can_edit_and_copy_output_for_next_agent": "ä½ å¯ä»¥ç·¨è¼¯ä¸Šæ–¹è¼¸å‡ºï¼Œå†è¤‡è£½æˆä¸‹ä¸€å€‹ä»£ç†äººçš„è¼¸å…¥ã€‚",
        "ai_key_from_env": "å·²å¾ä¼ºæœå™¨ç’°å¢ƒè¼‰å…¥ API Keyã€‚",
        "ai_key_input": "è«‹è¼¸å…¥ API Keyï¼ˆåªæœƒå­˜åœ¨æ­¤ç€è¦½å™¨å·¥ä½œéšæ®µï¼‰",
        "language": "ä»‹é¢èªè¨€",
        "light_mode": "æ·ºè‰²æ¨¡å¼",
        "dark_mode": "æ·±è‰²æ¨¡å¼",
        # AI Note Keeper
        "note_keeper_title": "AI ç­†è¨˜ç®¡å®¶",
        "note_raw_input": "è²¼ä¸ŠåŸå§‹æ–‡å­—ç­†è¨˜",
        "note_to_markdown": "AI è½‰æ›ç‚º Markdown ç­†è¨˜",
        "note_edit_mode": "ç­†è¨˜ç·¨è¼¯ / æª¢è¦–æ¨¡å¼",
        "note_edit_text": "æ–‡å­—ç·¨è¼¯æ¨¡å¼",
        "note_preview_md": "Markdown é è¦½æ¨¡å¼",
        "current_note_md": "ç›®å‰ Markdown ç­†è¨˜",
        "ai_formatting": "AI æ’ç‰ˆå„ªåŒ–",
        "ai_formatting_desc": "ä½¿ç”¨ AI é‡æ–°æ•´ç†æ¨™é¡Œã€æ®µè½èˆ‡é …ç›®ç¬¦è™Ÿï¼Œä½†ä¸æ”¹è®Šäº‹å¯¦å…§å®¹ã€‚",
        "ai_keywords": "AI é—œéµå­—æ¨™è¨»",
        "ai_keywords_desc": "è¼¸å…¥æ¬²æ¨™è¨»çš„é—œéµå­—èˆ‡é¡è‰²ï¼Œç³»çµ±æœƒåœ¨ç­†è¨˜ä¸­é«˜äº®é¡¯ç¤ºã€‚",
        "keywords_input": "è¼¸å…¥é—œéµå­—ï¼ˆä»¥é€—è™Ÿæˆ–æ›è¡Œåˆ†éš”ï¼‰",
        "keywords_color": "é¸æ“‡é—œéµå­—é«˜äº®é¡è‰²",
        "apply_keywords": "å¥—ç”¨é—œéµå­—æ¨™è¨»",
        "ai_entities": "AI å¯¦é«”æ“·å– (20 ç­†)",
        "ai_entities_desc": "ç”± AI å¾ç­†è¨˜ä¸­æ“·å– 20 å€‹é—œéµå¯¦é«”ï¼Œä¸¦ä»¥ Markdown è¡¨æ ¼å‘ˆç¾ã€‚",
        "run_entities_extraction": "ç”¢ç”Ÿå¯¦é«”è¡¨æ ¼",
        "ai_chat": "AI ç­†è¨˜å°è©±",
        "ai_chat_prompt": "è«‹è¼¸å…¥ä½ çš„å•é¡Œæˆ–æŒ‡ä»¤ï¼ˆæœƒä»¥ç›®å‰ç­†è¨˜ç‚ºä¸Šä¸‹æ–‡ï¼‰",
        "ai_chat_run": "é€å‡ºå°è©±",
        "ai_summary": "AI æ‘˜è¦",
        "ai_summary_prompt": "è‡ªè¨‚æ‘˜è¦ Promptï¼ˆå¯ä½¿ç”¨ç¹é«”ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰",
        "ai_summary_run": "ç”¢ç”Ÿæ‘˜è¦",
        "ai_magics": "AI Magics",
        "ai_magic_1": "AI é¢¨éšªé›·é”",
        "ai_magic_1_desc": "å¾ç­†è¨˜ä¸­æ‰¾å‡ºæ½›åœ¨é¢¨éšªæƒ…å¢ƒèˆ‡é‡è¦è­¦è¨Šã€‚",
        "ai_magic_1_run": "ç”¢ç”Ÿé¢¨éšªé›·é”å ±å‘Š",
        "ai_magic_2": "AI è¡Œå‹•è—åœ–",
        "ai_magic_2_desc": "æ ¹æ“šç­†è¨˜å…§å®¹ç”¢ç”Ÿåˆ†éšæ®µè¡Œå‹•å»ºè­°èˆ‡å„ªå…ˆé †åºã€‚",
        "ai_magic_2_run": "ç”¢ç”Ÿè¡Œå‹•è—åœ–",
        "model_select": "é¸æ“‡æ¨¡å‹",
        "tokens_input": "Max Tokensï¼ˆé è¨­ 12000ï¼‰",
    },
    "en": {
        # å¯è¦–éœ€è¦è£œé½Šè‹±æ–‡å°ç…§
    },
}


def t(key: str) -> str:
    lang = st.session_state.get("language", "zh-TW")
    return TRANSLATIONS.get(lang, TRANSLATIONS["zh-TW"]).get(key, key)


# =========================
# èŠ±å‰ä¸»é¡Œ (20 ç¨® Jackslot ä¸»é¡Œï¼Œç°¡åŒ–åƒæ•¸)
# =========================

FLOWER_THEMES = [
    {
        "id": "sakura_mist",
        "name_zh": "æ«»èŠ±è–„éœ§",
        "name_en": "Sakura Mist",
        "emoji": "ğŸŒ¸",
        "bg_light": "#fff7fb",
        "bg_dark": "#2b1b2f",
        "primary": "#f48fb1",
        "accent": "#ff80ab",
    },
    {
        "id": "lotus_serenity",
        "name_zh": "è“®èŠ±éœå¿ƒ",
        "name_en": "Lotus Serenity",
        "emoji": "ğŸŒº",
        "bg_light": "#f5fbff",
        "bg_dark": "#102027",
        "primary": "#80cbc4",
        "accent": "#26a69a",
    },
    {
        "id": "iris_dusk",
        "name_zh": "é³¶å°¾æš®å…‰",
        "name_en": "Iris Dusk",
        "emoji": "ğŸŒ¸",
        "bg_light": "#f3f2ff",
        "bg_dark": "#1c1b2e",
        "primary": "#7e57c2",
        "accent": "#9575cd",
    },
    {
        "id": "rose_gold",
        "name_zh": "ç«ç‘°æ™¨å…‰",
        "name_en": "Rose Gold",
        "emoji": "ğŸŒ¹",
        "bg_light": "#fff4f4",
        "bg_dark": "#32131a",
        "primary": "#ef5350",
        "accent": "#ff8a80",
    },
    {
        "id": "orchid_neon",
        "name_zh": "è˜­èŠ±éœ“è™¹",
        "name_en": "Orchid Neon",
        "emoji": "ğŸ’®",
        "bg_light": "#faf5ff",
        "bg_dark": "#2a1635",
        "primary": "#ba68c8",
        "accent": "#e1bee7",
    },
    {
        "id": "sunflower_field",
        "name_zh": "å‘æ—¥è‘µåŸé‡",
        "name_en": "Sunflower Field",
        "emoji": "ğŸŒ»",
        "bg_light": "#fffde7",
        "bg_dark": "#322b0a",
        "primary": "#fbc02d",
        "accent": "#ffeb3b",
    },
    {
        "id": "lavender_breeze",
        "name_zh": "è–°è¡£è‰å¾®é¢¨",
        "name_en": "Lavender Breeze",
        "emoji": "ğŸ’",
        "bg_light": "#f8f4ff",
        "bg_dark": "#241b38",
        "primary": "#9575cd",
        "accent": "#b39ddb",
    },
    {
        "id": "camellia_silk",
        "name_zh": "å±±èŒ¶çµ²ç¶¢",
        "name_en": "Camellia Silk",
        "emoji": "ğŸŒº",
        "bg_light": "#fff8f6",
        "bg_dark": "#2d1c16",
        "primary": "#ff7043",
        "accent": "#ffab91",
    },
    {
        "id": "peony_glow",
        "name_zh": "ç‰¡ä¸¹æµå…‰",
        "name_en": "Peony Glow",
        "emoji": "ğŸŒ¸",
        "bg_light": "#fff0f6",
        "bg_dark": "#3a1024",
        "primary": "#ec407a",
        "accent": "#f48fb1",
    },
    {
        "id": "cherry_blossom_night",
        "name_zh": "å¤œæ«»å¾®é›¨",
        "name_en": "Cherry Blossom Night",
        "emoji": "ğŸŒ¸",
        "bg_light": "#fef5ff",
        "bg_dark": "#1e1325",
        "primary": "#f06292",
        "accent": "#ce93d8",
    },
    {
        "id": "magnolia_morning",
        "name_zh": "ç‰è˜­æ™¨éœ²",
        "name_en": "Magnolia Morning",
        "emoji": "ğŸŒ¼",
        "bg_light": "#f8fff9",
        "bg_dark": "#102019",
        "primary": "#aed581",
        "accent": "#c5e1a5",
    },
    {
        "id": "hydrangea_rain",
        "name_zh": "ç¹¡çƒé›¨é„",
        "name_en": "Hydrangea Rain",
        "emoji": "ğŸŒ¸",
        "bg_light": "#f3f8ff",
        "bg_dark": "#131c2e",
        "primary": "#64b5f6",
        "accent": "#90caf9",
    },
    {
        "id": "poppy_fire",
        "name_zh": "ç½Œç²Ÿçƒˆç„°",
        "name_en": "Poppy Fire",
        "emoji": "ğŸŒº",
        "bg_light": "#fff3e0",
        "bg_dark": "#3b1b0b",
        "primary": "#ff7043",
        "accent": "#ffab91",
    },
    {
        "id": "daisy_cloud",
        "name_zh": "é››èŠé›²å…‰",
        "name_en": "Daisy Cloud",
        "emoji": "ğŸŒ¼",
        "bg_light": "#f9fff6",
        "bg_dark": "#1b2512",
        "primary": "#c0ca33",
        "accent": "#dce775",
    },
    {
        "id": "lotus_moon",
        "name_zh": "æœˆè‰²è·å¡˜",
        "name_en": "Lotus Moon",
        "emoji": "ğŸŒ¸",
        "bg_light": "#f5fff9",
        "bg_dark": "#101f19",
        "primary": "#4db6ac",
        "accent": "#80cbc4",
    },
    {
        "id": "iris_frost",
        "name_zh": "éœœæŸ“é³¶å°¾",
        "name_en": "Iris Frost",
        "emoji": "ğŸ’",
        "bg_light": "#f4f7ff",
        "bg_dark": "#151c2f",
        "primary": "#5c6bc0",
        "accent": "#7986cb",
    },
    {
        "id": "rose_noir",
        "name_zh": "é»‘ç«ç‘°åºæ›²",
        "name_en": "Rose Noir",
        "emoji": "ğŸŒ¹",
        "bg_light": "#fdf2f5",
        "bg_dark": "#1f0b12",
        "primary": "#d32f2f",
        "accent": "#e57373",
    },
    {
        "id": "orchid_ice",
        "name_zh": "è˜­å†°æ™¨æ›²",
        "name_en": "Orchid Ice",
        "emoji": "ğŸ’®",
        "bg_light": "#faf3ff",
        "bg_dark": "#231a33",
        "primary": "#ab47bc",
        "accent": "#ce93d8",
    },
    {
        "id": "sunrise_tulip",
        "name_zh": "æ›™å…‰é¬±é‡‘é¦™",
        "name_en": "Sunrise Tulip",
        "emoji": "ğŸŒ·",
        "bg_light": "#fff6e8",
        "bg_dark": "#3a2312",
        "primary": "#ff8f00",
        "accent": "#ffb74d",
    },
    {
        "id": "garden_mint",
        "name_zh": "åº­åœ’è–„è·",
        "name_en": "Garden Mint",
        "emoji": "ğŸŒ¿",
        "bg_light": "#f3fff9",
        "bg_dark": "#10241b",
        "primary": "#4caf50",
        "accent": "#81c784",
    },
]


def get_theme_by_id(theme_id: str) -> Dict[str, Any]:
    for th in FLOWER_THEMES:
        if th["id"] == theme_id:
            return th
    return FLOWER_THEMES[0]


def get_theme_name(theme: Dict[str, Any]) -> str:
    lang = st.session_state.get("language", "zh-TW")
    return theme["name_zh"] if lang == "zh-TW" else theme["name_en"]


def apply_theme_css():
    theme_id = st.session_state.get("theme_id", FLOWER_THEMES[0]["id"])
    theme = get_theme_by_id(theme_id)
    dark_mode = st.session_state.get("dark_mode", False)

    bg = theme["bg_dark"] if dark_mode else theme["bg_light"]
    primary = theme["primary"]
    accent = theme["accent"]

    css = f"""
    <style>
    body {{
        background-color: {bg} !important;
    }}
    .stApp {{
        background-color: {bg} !important;
    }}
    .css-18e3th9, .css-1d391kg {{
        background-color: {bg} !important;
    }}
    .stButton>button {{
        border-radius: 999px;
        border: 1px solid {accent};
        color: #ffffff;
        background: linear-gradient(90deg, {primary}, {accent});
    }}
    .stTabs [data-baseweb="tab"] {{
        font-weight: 600;
    }}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)


# =========================
# LLM Provider èˆ‡æ¨¡å‹è¨­å®š
# =========================

MODEL_OPTIONS = [
    ("openai", "gpt-4o-mini"),
    ("openai", "gpt-4.1-mini"),
    ("gemini", "gemini-2.5-flash"),
    ("gemini", "gemini-2.5-flash-lite"),
    ("anthropic", "claude-3-5-sonnet"),
    ("anthropic", "claude-3-5-haiku"),
    ("grok", "grok-4-fast-reasoning"),
    ("grok", "grok-3-mini"),
]


def get_model_label(provider: str, model: str) -> str:
    return f"{provider}:{model}"


def parse_model_label(label: str):
    provider, model = label.split(":", 1)
    return provider, model


# =========================
# API Key ç®¡ç†ï¼ˆç’°å¢ƒè®Šæ•¸å„ªå…ˆï¼‰
# =========================

ENV_KEYS = {
    "gemini": os.getenv("GEMINI_API_KEY"),
    "openai": os.getenv("OPENAI_API_KEY"),
    "anthropic": os.getenv("ANTHROPIC_API_KEY"),
    "grok": os.getenv("GROK_API_KEY"),
}


def init_session():
    if "language" not in st.session_state:
        st.session_state["language"] = "zh-TW"
    if "dark_mode" not in st.session_state:
        st.session_state["dark_mode"] = False
    if "theme_id" not in st.session_state:
        st.session_state["theme_id"] = FLOWER_THEMES[0]["id"]
    if "user_keys" not in st.session_state:
        st.session_state["user_keys"] = {}
    if "records" not in st.session_state:
        st.session_state["records"] = None
    if "agent_outputs" not in st.session_state:
        st.session_state["agent_outputs"] = {}
    if "note_raw" not in st.session_state:
        st.session_state["note_raw"] = ""
    if "note_md" not in st.session_state:
        st.session_state["note_md"] = ""
    if "note_edit_mode" not in st.session_state:
        st.session_state["note_edit_mode"] = "text"


def get_active_api_key(provider: str) -> str | None:
    if ENV_KEYS.get(provider):
        return ENV_KEYS[provider]
    return st.session_state["user_keys"].get(provider)


# =========================
# LLM å‘¼å« (éœ€è‡ªè¡Œå¯¦ä½œå…·é«” API å‘¼å«)
# =========================

def run_llm(provider: str, model: str, prompt: str, max_tokens: int = 12000) -> str:
    """
    å¯¦å‹™ä¸­è«‹åœ¨é€™è£¡æ¥ OpenAI / Gemini / Anthropic / Grok çš„ SDK æˆ– HTTPã€‚
    é€™è£¡å…ˆç”¨ placeholderï¼Œé¿å…éƒ¨ç½²æ™‚å‡ºéŒ¯ã€‚
    """
    api_key = get_active_api_key(provider)
    if not api_key:
        return f"[éŒ¯èª¤] å°šæœªè¨­å®š {provider} çš„ API Keyï¼Œç„¡æ³•å‘¼å«æ¨¡å‹ {model}ã€‚"

    # TODO: æ”¹ç‚ºå¯¦éš› API å‘¼å«
    fake_response = f"(æ¨¡æ“¬ {provider}:{model} å›æ‡‰)\n\n" + prompt[:2000]
    return fake_response


# =========================
# è®€å– agents.yaml
# =========================

def load_agents_config() -> List[Dict[str, Any]]:
    try:
        with open("agents.yaml", "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)
        return cfg.get("agents", [])
    except Exception as e:
        st.error(f"è®€å– agents.yaml å¤±æ•—: {e}")
        return []


# =========================
# UI å€å¡Šï¼šé ‚éƒ¨åˆ— & Sidebar
# =========================

def render_topbar():
    cols = st.columns([4, 2, 2])
    with cols[0]:
        st.markdown(f"## {t('title')}")
    with cols[1]:
        st.write("")  # å ä½
        lang = st.radio(
            t("language"),
            ["zh-TW", "en"],
            horizontal=True,
            index=0 if st.session_state["language"] == "zh-TW" else 1,
            key="lang_radio_top",
        )
        st.session_state["language"] = lang
    with cols[2]:
        st.write("")
        dark = st.checkbox(t("dark_mode"), value=st.session_state["dark_mode"])
        st.session_state["dark_mode"] = dark


def render_theme_jackslot():
    st.subheader(t("theme_selector"))

    current_theme = get_theme_by_id(st.session_state["theme_id"])
    st.markdown(
        f"**{current_theme['emoji']} {get_theme_name(current_theme)}**",
        unsafe_allow_html=True,
    )

    col1, col2 = st.columns(2)
    with col1:
        if st.button(t("spin_random_theme")):
            random_theme = random.choice(FLOWER_THEMES)
            st.session_state["theme_candidate"] = random_theme["id"]
    with col2:
        if st.button(t("apply_theme")):
            candidate_id = st.session_state.get("theme_candidate", current_theme["id"])
            st.session_state["theme_id"] = candidate_id

    # ç›´æ¥é¸æ“‡ä¸»é¡Œ
    options_labels = [get_theme_name(th) for th in FLOWER_THEMES]
    options_ids = [th["id"] for th in FLOWER_THEMES]
    try:
        idx = options_ids.index(st.session_state["theme_id"])
    except ValueError:
        idx = 0
    selected_label = st.selectbox(t("select_theme"), options_labels, index=idx)
    st.session_state["theme_id"] = options_ids[options_labels.index(selected_label)]


def render_sidebar():
    with st.sidebar:
        # ä¸»é¡Œé¸æ“‡
        render_theme_jackslot()
        st.markdown("---")

        # API Keys
        st.subheader(t("api_keys"))

        for provider, label in [
            ("gemini", "Gemini"),
            ("openai", "OpenAI"),
            ("anthropic", "Anthropic"),
            ("grok", "Grok"),
        ]:
            st.markdown(f"**{label}**")
            if ENV_KEYS.get(provider):
                st.caption(t("ai_key_from_env"))
            else:
                key_val = st.text_input(
                    f"{label} {t('ai_key_input')}",
                    type="password",
                    key=f"{provider}_key_input",
                )
                if key_val:
                    st.session_state["user_keys"][provider] = key_val

        st.markdown("---")

        # è³‡æ–™é›† (ç°¡åŒ–ç‰ˆ)
        st.subheader(t("dataset"))
        upload = st.file_uploader(t("upload_csv"), type=["csv"], key="csv_uploader")
        if upload:
            import pandas as pd

            df = pd.read_csv(upload)
            st.session_state["records"] = df
            st.success(f"å·²è¼‰å…¥ {len(df)} ç­†è³‡æ–™ã€‚")
        if st.button(t("or_use_mock")):
            # TODO: å¯¦éš›è®€å– data/mock.csv
            st.session_state["records"] = None
            st.info("Demo æ¨¡å¼ï¼šå°šæœªå¯¦ä½œå¯¦éš› mock.csv è¼‰å…¥ã€‚")


# =========================
# å„åˆ†é  UI
# =========================

def render_dashboard():
    st.markdown("### å„€è¡¨æ¿ (TODO: å¯¦ä½œæŒ‡æ¨™å¡èˆ‡åœ–è¡¨)")
    st.info("æ­¤è™•å¯æ”¾ç½®è³‡æ–™å“è³ªã€é¢¨éšªåˆ†æ•¸ã€AI åŸ·è¡Œç‹€æ…‹ç­‰ WOW æŒ‡æ¨™ã€‚")


def render_data_studio():
    st.markdown("### è³‡æ–™å·¥ä½œå®¤")
    import pandas as pd

    df = st.session_state.get("records")
    if df is None:
        st.warning("å°šæœªè¼‰å…¥ä»»ä½•è³‡æ–™ã€‚è«‹åœ¨å·¦å´ä¸Šå‚³ CSV æˆ–ä½¿ç”¨ç¤ºç¯„è³‡æ–™ã€‚")
        return

    editable_df = st.data_editor(df, num_rows="dynamic", key="data_editor_df")
    st.session_state["records"] = editable_df

    # ä¸‹è¼‰
    csv_bytes = editable_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ä¸‹è¼‰ç›®å‰è³‡æ–™ç‚º CSV",
        data=csv_bytes,
        file_name="biochain_export.csv",
        mime="text/csv",
    )


def render_network_graph():
    st.markdown("### ä¾›æ‡‰éˆç¶²è·¯åœ–")
    st.info("æ­¤è™•å¯åµŒå…¥ D3.js åŠ›å°å‘åœ– (ä½¿ç”¨ st.components.v1.html)ã€‚ç›®å‰ç‚ºå ä½ã€‚")


def render_time_trends():
    st.markdown("### æ™‚é–“åºåˆ—è¶¨å‹¢åœ–")
    st.info("æ­¤è™•å¯ä½¿ç”¨ Plotly/Altair å‘ˆç¾å‡ºè²¨é‡éš¨æ™‚é–“è®ŠåŒ–ã€‚")


# -------------------------
# AI Agents Console
# -------------------------

def render_agents_console():
    st.markdown("### " + t("ai_agents_pipeline"))

    agents_cfg = load_agents_config()
    if not agents_cfg:
        st.warning("æ‰¾ä¸åˆ° agents.yaml æˆ–å…§å®¹ç‚ºç©ºã€‚")
        return

    exec_mode = st.radio(
        t("execution_mode"),
        [t("run_all_enabled"), t("run_single")],
        horizontal=True,
        key="agent_exec_mode",
    )

    run_all = exec_mode == t("run_all_enabled")

    previous_output = None
    for agent in agents_cfg:
        previous_output = render_single_agent_card(agent, previous_output, run_all)


def render_single_agent_card(agent: Dict[str, Any], previous_output: str | None, run_all: bool):
    agent_id = agent["id"]
    with st.expander(f"{agent['name']} ({agent['role']})", expanded=False):
        enabled = st.checkbox(
            t("enable_agent"),
            value=True,
            key=f"enable_{agent_id}",
        )

        if not enabled:
            return previous_output

        default_prompt = agent.get("default_prompt", "")
        prompt = st.text_area(
            t("prompt_template"),
            value=st.session_state.get(f"prompt_{agent_id}", default_prompt),
            key=f"prompt_{agent_id}",
            height=150,
        )

        # æ¨¡å‹é¸æ“‡
        default_label = get_model_label(
            agent.get("default_provider", "gemini"),
            agent.get("default_model", "gemini-2.5-flash"),
        )
        model_labels = [get_model_label(p, m) for p, m in MODEL_OPTIONS]
        if default_label not in model_labels:
            model_labels.insert(0, default_label)

        model_label = st.selectbox(
            t("model_select"),
            model_labels,
            index=model_labels.index(default_label),
            key=f"model_{agent_id}",
        )
        provider, model_name = parse_model_label(model_label)

        max_tokens = st.number_input(
            t("tokens_input"),
            min_value=512,
            max_value=120000,
            value=agent.get("default_max_tokens", 12000),
            step=512,
            key=f"max_tokens_{agent_id}",
        )

        # ä¸Šä¸€å€‹ Agent è¼¸å‡º
        if previous_output:
            if st.button(t("use_prev_output_as_input"), key=f"use_prev_{agent_id}"):
                st.session_state[f"input_from_prev_{agent_id}"] = previous_output

        input_text = st.text_area(
            t("agent_input"),
            value=st.session_state.get(f"input_from_prev_{agent_id}", ""),
            key=f"agent_input_{agent_id}",
            height=120,
        )

        # åŸ·è¡Œ
        run_button_pressed = False
        if run_all:
            run_button_pressed = enabled  # pipeline æ¨¡å¼ï¼šè‡ªå‹•åŸ·è¡Œå·²å•Ÿç”¨
        else:
            run_button_pressed = st.button(
                t("run_agent"),
                key=f"run_{agent_id}",
            )

        if run_button_pressed:
            full_prompt = f"{prompt}\n\n=== è¼¸å…¥è³‡æ–™ ===\n{input_text}"
            with st.spinner(f"åŸ·è¡Œ {agent['name']} ä¸­..."):
                output = run_llm(provider, model_name, full_prompt, max_tokens)
            st.session_state["agent_outputs"][agent_id] = output

        view_mode = st.radio(
            t("view_mode"),
            [t("plain_text"), t("markdown")],
            horizontal=True,
            key=f"view_mode_{agent_id}",
        )

        output = st.session_state["agent_outputs"].get(agent_id, "")
        if output:
            if view_mode == t("markdown"):
                st.markdown(output)
            else:
                st.text_area(
                    t("agent_output"),
                    value=output,
                    height=200,
                    key=f"output_text_{agent_id}",
                )
            st.caption(t("you_can_edit_and_copy_output_for_next_agent"))

        return output if output else previous_output


# -------------------------
# AI Note Keeper
# -------------------------

def render_ai_note_keeper():
    st.markdown(f"### {t('note_keeper_title')}")

    # åŸå§‹è²¼ä¸Šå€
    st.text_area(
        t("note_raw_input"),
        value=st.session_state["note_raw"],
        key="note_raw",
        height=200,
    )

    # è½‰ Markdown
    cols = st.columns([1, 2, 2])
    with cols[0]:
        if st.button(t("note_to_markdown")):
            provider, model = "gemini", "gemini-2.5-flash"
            prompt = (
                "è«‹å°‡ä»¥ä¸‹æ–‡å­—è½‰æ›ç‚ºçµæ§‹è‰¯å¥½çš„ Markdown ç­†è¨˜ï¼Œ"
                "é©åº¦åŠ å…¥æ¨™é¡Œ (##)ã€å­æ¨™é¡Œ (###)ã€æ¢åˆ—èˆ‡è¡¨æ ¼ï¼Œ"
                "ä½†ä¸è¦ä¸»è§€æ–°å¢äº‹å¯¦æˆ–åˆªé™¤é—œéµè³‡è¨Šã€‚\n\n"
                "=== åŸå§‹æ–‡å­— ===\n"
                + st.session_state["note_raw"]
            )
            with st.spinner("AI æ­£åœ¨å°‡æ–‡å­—è½‰ç‚º Markdown ..."):
                md = run_llm(provider, model, prompt, 4000)
            st.session_state["note_md"] = md

    with cols[1]:
        mode = st.radio(
            t("note_edit_mode"),
            [t("note_edit_text"), t("note_preview_md")],
            horizontal=True,
            key="note_edit_mode_radio",
        )
        st.session_state["note_edit_mode"] = (
            "text" if mode == t("note_edit_text") else "markdown"
        )

    with cols[2]:
        st.write("")

    # ç·¨è¼¯ / é è¦½
    if st.session_state["note_edit_mode"] == "text":
        st.session_state["note_md"] = st.text_area(
            t("current_note_md"),
            value=st.session_state["note_md"],
            key="note_md_edit",
            height=300,
        )
    else:
        st.markdown(st.session_state["note_md"] or "_ç›®å‰å°šæœªæœ‰ Markdown å…§å®¹_")

    st.markdown("---")

    # AI å·¥å…·å€ï¼šå››å¤§å·¥å…· + 2 å€‹ Magic
    col_fmt, col_kw, col_ent = st.columns(3)

    # --- AI Formatting ---
    with col_fmt:
        st.markdown(f"#### {t('ai_formatting')}")
        st.caption(t("ai_formatting_desc"))
        fmt_model_label = st.selectbox(
            t("model_select") + " (Formatting)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="fmt_model",
        )
        fmt_provider, fmt_model = parse_model_label(fmt_model_label)
        fmt_tokens = st.number_input(
            t("tokens_input") + " (Formatting)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="fmt_tokens",
        )
        if st.button(t("ai_formatting"), key="fmt_run"):
            prompt = (
                "è«‹åœ¨ä¸æ›´å‹•äº‹å¯¦å…§å®¹çš„å‰æä¸‹ï¼Œå°‡ä»¥ä¸‹ Markdown ç­†è¨˜é‡æ–°æ’ç‰ˆï¼š"
                "å„ªåŒ–æ¨™é¡Œå±¤ç´šã€æ¢åˆ—ç¬¦è™Ÿèˆ‡å¯è®€æ€§ï¼Œé©åº¦æ‹†åˆ†æ®µè½ã€‚\n\n"
                "=== Markdown ç­†è¨˜ ===\n"
                + st.session_state["note_md"]
            )
            with st.spinner("AI æ­£åœ¨æ’ç‰ˆå„ªåŒ–..."):
                md = run_llm(fmt_provider, fmt_model, prompt, fmt_tokens)
            st.session_state["note_md"] = md
            st.success("å·²å®Œæˆ AI æ’ç‰ˆå„ªåŒ–ã€‚")

    # --- AI Keywords ---
    with col_kw:
        st.markdown(f"#### {t('ai_keywords')}")
        st.caption(t("ai_keywords_desc"))
        keywords_str = st.text_area(
            t("keywords_input"),
            key="keywords_input",
            height=80,
        )
        color = st.color_picker(
            t("keywords_color"),
            value="#ffeb3b",
            key="keywords_color_picker",
        )
        if st.button(t("apply_keywords")):
            note_md = st.session_state["note_md"]
            if not note_md:
                st.warning("å°šç„¡ Markdown å…§å®¹å¯æ¨™è¨»ã€‚")
            else:
                keywords = []
                for part in keywords_str.replace("\n", ",").split(","):
                    w = part.strip()
                    if w:
                        keywords.append(w)
                for kw in sorted(set(keywords), key=len, reverse=True):
                    if kw in note_md:
                        note_md = note_md.replace(
                            kw,
                            f"<span style='background-color:{color};"
                            f"padding:0 2px;border-radius:2px'>{kw}</span>",
                        )
                st.session_state["note_md"] = note_md
                st.success("å·²å¥—ç”¨é—œéµå­—é«˜äº®ã€‚")
                st.markdown(
                    "ï¼ˆæ³¨æ„ï¼šé—œéµå­—é«˜äº®ä½¿ç”¨ HTML spanï¼Œéœ€è¦ Markdown é é¢å…è¨± HTMLï¼‰"
                )

    # --- AI Entities ---
    with col_ent:
        st.markdown(f"#### {t('ai_entities')}")
        st.caption(t("ai_entities_desc"))
        ent_model_label = st.selectbox(
            t("model_select") + " (Entities)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="ent_model",
        )
        ent_provider, ent_model = parse_model_label(ent_model_label)
        ent_tokens = st.number_input(
            t("tokens_input") + " (Entities)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="ent_tokens",
        )
        if st.button(t("run_entities_extraction")):
            prompt = (
                "ä½ æ˜¯ä¸€ä½é†«ç™‚ä¾›æ‡‰éˆèˆ‡æ³•è¦é ˜åŸŸçš„çŸ¥è­˜åœ–è­œå°ˆå®¶ã€‚"
                "è«‹å¾ä»¥ä¸‹ Markdown ç­†è¨˜ä¸­èƒå–æœ€å¤š 20 å€‹é—œéµå¯¦é«”ï¼Œ"
                "ä¸¦ä»¥ Markdown è¡¨æ ¼å‘ˆç¾ï¼Œæ¬„ä½åŒ…å«ï¼š"
                "å¯¦é«”åç¨±ã€å¯¦é«”é¡å‹ï¼ˆæ©Ÿæ§‹/é†«æ/æ³•è¦/é¢¨éšª/æ™‚é–“â€¦ï¼‰ã€"
                "ç›¸é—œä¸Šä¸‹æ–‡æ‘˜è¦ã€æ½›åœ¨é¢¨éšªç­‰ç´šï¼ˆä½/ä¸­/é«˜ï¼‰ã€å‚™è¨»ã€‚\n\n"
                "=== Markdown ç­†è¨˜ ===\n"
                + st.session_state["note_md"]
            )
            with st.spinner("AI æ­£åœ¨æ“·å–å¯¦é«”..."):
                ents_markdown = run_llm(ent_provider, ent_model, prompt, ent_tokens)
            st.markdown("##### å¯¦é«”è¡¨æ ¼")
            st.markdown(ents_markdown)

    st.markdown("---")

    # AI Chat + Summary + Magics
    col_chat, col_sum = st.columns(2)

    # --- AI Chat ---
    with col_chat:
        st.markdown(f"#### {t('ai_chat')}")
        chat_prompt = st.text_area(
            t("ai_chat_prompt"),
            key="note_chat_prompt",
            height=120,
        )
        chat_model_label = st.selectbox(
            t("model_select") + " (Chat)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="chat_model",
        )
        chat_provider, chat_model = parse_model_label(chat_model_label)
        chat_tokens = st.number_input(
            t("tokens_input") + " (Chat)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="chat_tokens",
        )
        if st.button(t("ai_chat_run")):
            prompt = (
                "ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…çš„å·¥ä½œç­†è¨˜ï¼ˆMarkdown æ ¼å¼ï¼‰ä½œç‚ºä¸Šä¸‹æ–‡ï¼Œ"
                "è«‹æ ¹æ“šç­†è¨˜å…§å®¹èˆ‡ä½¿ç”¨è€…å•é¡Œé€²è¡Œå°ˆæ¥­å›ç­”ï¼Œ"
                "å›ç­”æ™‚å¯å¼•ç”¨é—œéµç‰‡æ®µï¼Œä½†é¿å…æé€ ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚\n\n"
                "=== ç­†è¨˜å…§å®¹ ===\n"
                f"{st.session_state['note_md']}\n\n"
                "=== ä½¿ç”¨è€…å•é¡Œ ===\n"
                f"{chat_prompt}"
            )
            with st.spinner("AI æ­£åœ¨å›æ‡‰å°è©±..."):
                chat_resp = run_llm(chat_provider, chat_model, prompt, chat_tokens)
            st.markdown("##### å°è©±å›æ‡‰")
            st.markdown(chat_resp)

    # --- AI Summary ---
    with col_sum:
        st.markdown(f"#### {t('ai_summary')}")
        default_sum_prompt = (
            "è«‹ä»¥æ¢åˆ—æ–¹å¼æ’°å¯«æ­¤ç­†è¨˜çš„é‡é»æ‘˜è¦ï¼ŒåŒ…å«ï¼š\n"
            "1. æ ¸å¿ƒè­°é¡Œ\n"
            "2. ä¸»è¦åˆ©å®³é—œä¿‚äºº\n"
            "3. é‡è¦é¢¨éšªæˆ–è­¦è¨Š\n"
            "4. å»ºè­°å¾ŒçºŒå‹•ä½œï¼ˆå¦‚æœ‰ï¼‰\n"
        )
        sum_prompt = st.text_area(
            t("ai_summary_prompt"),
            value=default_sum_prompt,
            key="summary_prompt",
            height=150,
        )
        sum_model_label = st.selectbox(
            t("model_select") + " (Summary)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="sum_model",
        )
        sum_provider, sum_model = parse_model_label(sum_model_label)
        sum_tokens = st.number_input(
            t("tokens_input") + " (Summary)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="sum_tokens",
        )
        if st.button(t("ai_summary_run")):
            prompt = (
                sum_prompt
                + "\n\n=== ç­†è¨˜å…§å®¹ ===\n"
                + st.session_state["note_md"]
            )
            with st.spinner("AI æ­£åœ¨ç”¢ç”Ÿæ‘˜è¦..."):
                summary = run_llm(sum_provider, sum_model, prompt, sum_tokens)
            st.markdown("##### æ‘˜è¦çµæœ")
            st.markdown(summary)

    st.markdown("---")

    # --- AI Magics ---
    st.markdown(f"#### {t('ai_magics')}")
    col_m1, col_m2 = st.columns(2)

    # Magic 1: é¢¨éšªé›·é”
    with col_m1:
        st.markdown(f"##### {t('ai_magic_1')}")
        st.caption(t("ai_magic_1_desc"))
        m1_model_label = st.selectbox(
            t("model_select") + " (Risk Radar)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="m1_model",
        )
        m1_provider, m1_model = parse_model_label(m1_model_label)
        m1_tokens = st.number_input(
            t("tokens_input") + " (Risk Radar)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="m1_tokens",
        )
        if st.button(t("ai_magic_1_run")):
            prompt = (
                "ä½ æ˜¯ä¸€ä½é†«ç™‚å™¨æä¾›æ‡‰éˆèˆ‡åˆè¦é¢¨éšªå°ˆå®¶ã€‚"
                "è«‹å¾ä»¥ä¸‹ç­†è¨˜ä¸­è¾¨è­˜å¯èƒ½çš„é¢¨éšªæƒ…å¢ƒèˆ‡è­¦è¨Šï¼Œ"
                "ä¸¦ä»¥ Markdown æ¢åˆ—èˆ‡è¡¨æ ¼å½¢å¼è¼¸å‡ºï¼š"
                "åŒ…å«é¢¨éšªé¡å‹ã€å¯èƒ½å½±éŸ¿ã€ç™¼ç”Ÿæ©Ÿç‡ã€é¢¨éšªç­‰ç´šèˆ‡å»ºè­°æ§ç®¡æªæ–½ã€‚\n\n"
                "=== ç­†è¨˜å…§å®¹ ===\n"
                + st.session_state["note_md"]
            )
            with st.spinner("AI æ­£åœ¨ç”¢ç”Ÿé¢¨éšªé›·é”..."):
                risk_report = run_llm(m1_provider, m1_model, prompt, m1_tokens)
            st.markdown("##### é¢¨éšªé›·é”å ±å‘Š")
            st.markdown(risk_report)

    # Magic 2: è¡Œå‹•è—åœ–
    with col_m2:
        st.markdown(f"##### {t('ai_magic_2')}")
        st.caption(t("ai_magic_2_desc"))
        m2_model_label = st.selectbox(
            t("model_select") + " (Action Blueprint)",
            [get_model_label(p, m) for p, m in MODEL_OPTIONS],
            key="m2_model",
        )
        m2_provider, m2_model = parse_model_label(m2_model_label)
        m2_tokens = st.number_input(
            t("tokens_input") + " (Action Blueprint)",
            min_value=512,
            max_value=120000,
            value=12000,
            step=512,
            key="m2_tokens",
        )
        if st.button(t("ai_magic_2_run")):
            prompt = (
                "è«‹æ‰®æ¼”ä¸€ä½é†«ç™‚å“è³ªèˆ‡æµç¨‹æ”¹å–„é¡§å•ï¼Œ"
                "æ ¹æ“šä»¥ä¸‹ç­†è¨˜å…§å®¹ï¼Œè¨­è¨ˆä¸€ä»½ã€è¡Œå‹•è—åœ–ã€ï¼š"
                "åˆ†ç‚ºçŸ­æœŸï¼ˆ1-3 å€‹æœˆï¼‰ã€ä¸­æœŸï¼ˆ3-12 å€‹æœˆï¼‰ã€é•·æœŸï¼ˆè¶…é 1 å¹´ï¼‰ï¼Œ"
                "æ¯å€‹éšæ®µä»¥è¡¨æ ¼åˆ—å‡ºï¼šè¡Œå‹•é …ç›®ã€è² è²¬è§’è‰²ã€é æœŸæˆæœèˆ‡é¢¨éšªæé†’ã€‚\n\n"
                "=== ç­†è¨˜å…§å®¹ ===\n"
                + st.session_state["note_md"]
            )
            with st.spinner("AI æ­£åœ¨ç”¢ç”Ÿè¡Œå‹•è—åœ–..."):
                plan = run_llm(m2_provider, m2_model, prompt, m2_tokens)
            st.markdown("##### è¡Œå‹•è—åœ–")
            st.markdown(plan)


def render_system_logs():
    st.markdown("### ç³»çµ±èˆ‡ç´€éŒ„ (TODO)")
    st.info("æ­¤è™•å¯é¡¯ç¤º AI åŸ·è¡Œ logã€éŒ¯èª¤è¨Šæ¯ã€ç‰ˆæœ¬è³‡è¨Šç­‰ã€‚")


# =========================
# ä¸»ç¨‹å¼
# =========================

def main():
    init_session()
    apply_theme_css()

    render_topbar()
    render_sidebar()

    tabs = st.tabs(
        [
            t("dashboard"),
            t("data_studio"),
            t("network_graph"),
            t("time_trends"),
            t("ai_agents"),
            t("ai_note_keeper"),
            t("system_logs"),
        ]
    )

    with tabs[0]:
        render_dashboard()
    with tabs[1]:
        render_data_studio()
    with tabs[2]:
        render_network_graph()
    with tabs[3]:
        render_time_trends()
    with tabs[4]:
        render_agents_console()
    with tabs[5]:
        render_ai_note_keeper()
    with tabs[6]:
        render_system_logs()


if __name__ == "__main__":
    main()
```

---

## 2. `agents.yaml`ï¼ˆ31 å€‹é€²éšä»£ç†äººï¼Œç¹é«”ä¸­æ–‡ï¼‰

```yaml
agents:
  - id: auditor
    name: "ç¨½æ ¸å¯©æŸ¥å“¡"
    role: "Auditor"
    description: "é‡å°é†«ç™‚å™¨æäº¤æ˜“è³‡æ–™é€²è¡Œç•°å¸¸åµæ¸¬èˆ‡ç¨½æ ¸å»ºè­°ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½é†«ç™‚å™¨æä¾›æ‡‰éˆçš„è³‡æ·±ç¨½æ ¸å°ˆå®¶ã€‚
      è«‹ä»¥åš´è¬¹ä½†æ¸…æ¥šæ˜“æ‡‚çš„æ–¹å¼æª¢è¦–çµ¦å®šçš„äº¤æ˜“è³‡æ–™ï¼ˆå·²æ•´ç†ç‚ºæ–‡å­—æˆ–è¡¨æ ¼æ‘˜è¦ï¼‰ï¼Œ
      é‡å°ä»¥ä¸‹é¢å‘æå‡ºåˆ†æï¼š
      1. æ•¸é‡ç•°å¸¸ï¼ˆä¾‹å¦‚ç•°å¸¸æ”¾å¤§ã€ç•°å¸¸ç¸®å°ã€è² æ•¸ã€æ¥µç«¯å€¼ï¼‰
      2. æ™‚é–“åˆ†ä½ˆç•°å¸¸ï¼ˆé›†ä¸­åœ¨ç‰¹å®šæ—¥æœŸã€å‡æ—¥ã€éå·¥ä½œæ™‚æ®µï¼‰
      3. ä¾›æ‡‰å•†èˆ‡é†«ç™‚é™¢æ‰€ä¹‹é–“çš„ç•°å¸¸é—œä¿‚ï¼ˆäº¤æ˜“éåº¦é›†ä¸­ã€é »ç‡ç•°å¸¸ï¼‰
      4. å¯èƒ½æ¶‰åŠæ³•è¦æˆ–åˆç´„é•åçš„æƒ…å¢ƒ
      è«‹ä»¥æ¢åˆ—æ–¹å¼å‘ˆç¾ç™¼ç¾èˆ‡å»ºè­°ï¼Œå¿…è¦æ™‚å¯é™„ä¸Šç¤ºæ„è¡¨æ ¼ã€‚

  - id: logistics_analyst
    name: "ç‰©æµè·¯å¾‘åˆ†æå¸«"
    role: "Logistics"
    description: "åˆ†æä¾›æ‡‰éˆè·¯å¾‘ã€ç¯€é»èˆ‡ç‰©æµæ•ˆç‡ç“¶é ¸ã€‚"
    default_provider: "openai"
    default_model: "gpt-4o-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«ç™‚å™¨æç‰©æµèˆ‡ä¾›æ‡‰éˆå„ªåŒ–å°ˆå®¶ã€‚
      æ ¹æ“šæä¾›çš„äº¤æ˜“é—œä¿‚èˆ‡æ•¸é‡è³‡è¨Šï¼Œè«‹ï¼š
      1. æè¿°æ•´é«”ä¾›æ‡‰éˆç¶²è·¯çµæ§‹ï¼ˆç¯€é»é¡å‹ã€ä¸»è¦è·¯å¾‘ã€æ¨ç´ç¯€é»ï¼‰
      2. æ‰¾å‡ºé«˜æµé‡ç“¶é ¸ç¯€é»èˆ‡å–®é»æ•…éšœé¢¨éšª
      3. æå‡ºå¯è¡Œçš„è·¯å¾‘å„ªåŒ–æˆ–å‚™æ´è¨­è¨ˆå»ºè­°
      4. æŒ‡å‡ºä»»ä½•èˆ‡å†·éˆã€æ™‚æ•ˆæ€§æˆ–åº«å­˜é€±è½‰ç›¸é—œçš„é¢¨éšª

  - id: legal_compliance
    name: "æ³•è¦åˆè¦é¡§å•"
    role: "Legal"
    description: "å¾æ³•è¦èˆ‡åˆç´„è§’åº¦æª¢è¦–è³‡æ–™èˆ‡æµç¨‹æ˜¯å¦åˆè¦ã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-sonnet"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰ TFDA èˆ‡åœ‹éš›æ³•è¦ï¼ˆå¦‚ EU MDRã€FDAï¼‰çš„æ³•è¦é¡§å•ã€‚
      è«‹æ ¹æ“šæä¾›çš„é†«ç™‚å™¨æäº¤æ˜“èˆ‡ç‰©æµæè¿°ï¼Œå›ç­”ä¸‹åˆ—å•é¡Œï¼š
      1. å“ªäº›è¡Œç‚ºæˆ–æ¨¡å¼å¯èƒ½é•åæ³•è¦æˆ–æŒ‡å¼•ï¼Ÿè«‹èªªæ˜åŸå› ã€‚
      2. å“ªäº›è³‡æ–™æ¬„ä½ç›®å‰ä¸è¶³ä»¥æ”¯æ´åˆè¦ç¨½æ ¸ï¼Ÿå»ºè­°è£œå¼·å“ªäº›ç´€éŒ„ï¼Ÿ
      3. è‹¥å‡ºç¾ç”¢å“ä¸è‰¯äº‹ä»¶æˆ–å¬å›ï¼Œç¾æœ‰ä¾›æ‡‰éˆç´€éŒ„æ˜¯å¦è¶³ä»¥è¿½æº¯ï¼Ÿ
      4. å»ºè­°çš„æ”¹å–„æªæ–½èˆ‡æ–‡ä»¶åŒ–éœ€æ±‚ã€‚

  - id: global_analyst
    name: "æ•´é«”æƒ…å¢ƒåˆ†æå¸«"
    role: "Analyst"
    description: "ç¶œåˆå„é¢å‘è¼¸å‡ºé«˜éšç®¡ç†å±¤å¯è®€çš„ç¸½çµå ±å‘Šã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½ç‚ºé«˜éšç®¡ç†å±¤æ’°å¯«ç°¡å ±æ‘˜è¦çš„ç­–ç•¥åˆ†æå¸«ã€‚
      å·²æœ‰å…¶ä»–ä»£ç†äººè¼¸å‡ºç¨½æ ¸ã€ç‰©æµã€æ³•è¦ç­‰åˆ†æã€‚
      è«‹æ•´åˆé€™äº›è¼¸å‡ºï¼Œæ’°å¯«ä¸€ä»½é©åˆæŠ•å½±ç‰‡æˆ–å ±å‘Šå°é¢çš„æ‘˜è¦ï¼Œå…§å®¹åŒ…å«ï¼š
      1. æ ¸å¿ƒç™¼ç¾ï¼ˆæœ€å¤š 5 é»ï¼‰
      2. é—œéµé¢¨éšªæˆ–æ©Ÿæœƒ
      3. å»ºè­°çš„çŸ­ä¸­é•·æœŸè¡Œå‹•
      4. å¦‚éœ€çµ¦ TFDA æˆ–é™¢æ–¹ç°¡å ±ï¼Œå¯ä½¿ç”¨çš„ç°¡çŸ­æ•˜è¿°ã€‚

  - id: data_quality_guard
    name: "è³‡æ–™å“è³ªå®ˆé–€å“¡"
    role: "Data Quality"
    description: "æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§ã€ä¸€è‡´æ€§èˆ‡å¯ç”¨æ€§ã€‚"
    default_provider: "grok"
    default_model: "grok-4-fast-reasoning"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯è³‡æ–™æ²»ç†ä¸»ç®¡ï¼Œå°ˆé–€æª¢æŸ¥é†«ç™‚äº¤æ˜“è³‡æ–™å“è³ªã€‚
      è«‹æ ¹æ“šæä¾›çš„æ¬„ä½èªªæ˜èˆ‡è³‡æ–™æ‘˜è¦ï¼Œåˆ†æï¼š
      1. ç¼ºå¤±å€¼ã€æ ¼å¼éŒ¯èª¤ã€é‡è¤‡ç´€éŒ„çš„æƒ…æ³
      2. æ¬„ä½é–“é‚è¼¯æ˜¯å¦ä¸€è‡´ï¼ˆä¾‹å¦‚æ•¸é‡æ‡‰ç‚ºæ­£æ•´æ•¸ã€æ—¥æœŸå€é–“åˆç†æ€§ï¼‰
      3. å°å¾ŒçºŒ AI åˆ†æèˆ‡æ³•è¦ç¨½æ ¸å¯èƒ½é€ æˆçš„å½±éŸ¿
      4. å»ºè­°çš„æ¸…ç†èˆ‡è£œå€¼ç­–ç•¥ã€‚

  - id: supply_risk_monitor
    name: "ä¾›æ‡‰é¢¨éšªç›£æ¸¬å“¡"
    role: "Supply Risk"
    description: "åµæ¸¬æ½›åœ¨ä¾›æ‡‰ä¸­æ–·ã€é›†ä¸­åº¦é¢¨éšªèˆ‡æ›¿ä»£æ–¹æ¡ˆã€‚"
    default_provider: "openai"
    default_model: "gpt-4.1-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¾›æ‡‰é¢¨éšªç®¡ç†å°ˆå®¶ã€‚
      è«‹å¾äº¤æ˜“è³‡æ–™èˆ‡ä¾›æ‡‰é—œä¿‚ä¸­ï¼Œæ‰¾å‡ºï¼š
      1. é«˜åº¦ä¾è³´å–®ä¸€ä¾›æ‡‰å•†æˆ–é†«é™¢çš„æƒ…å½¢
      2. æœ‰æ½›åœ¨ä¸­æ–·é¢¨éšªçš„è·¯å¾‘æˆ–ç¯€é»
      3. å»ºè­°çš„å‚™æ´ä¾›æ‡‰ä¾†æºæˆ–å¤šå…ƒåŒ–ç­–ç•¥
      4. è‹¥ç™¼ç”Ÿé‡å¤§å¬å›ï¼Œå°æ•´é«”ç¶²è·¯çš„å½±éŸ¿è©•ä¼°ã€‚

  - id: fraud_detector
    name: "ç•°å¸¸èˆ‡è©æ¬ºåµæ¸¬å™¨"
    role: "Fraud Detection"
    description: "åµæ¸¬å¯èƒ½èˆ‡è©æ¬ºã€æ´—è²¨æˆ–ä¸ç•¶ä½¿ç”¨ç›¸é—œçš„ç•°å¸¸æ¨¡å¼ã€‚"
    default_provider: "grok"
    default_model: "grok-3-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½å°ˆæ³¨æ–¼é†«ç™‚ä¾›æ‡‰éˆè©æ¬ºåµæ¸¬çš„è³‡æ–™åˆ†æå¸«ã€‚
      è«‹åˆ†æäº¤æ˜“è¨˜éŒ„ä¸­æ˜¯å¦å­˜åœ¨ï¼š
      1. ä¸å°‹å¸¸çš„äº¤æ˜“é‡å°–å³°æˆ–è°·åº•
      2. ä¸åˆç†çš„è·¯å¾‘ï¼ˆä¾‹å¦‚ç¹è·¯å¤šæ¬¡å†å›åˆ°åŸé»ï¼‰
      3. å¯èƒ½æ¶‰åŠäººç‚ºæ“ä½œæˆ–åˆ†æ‹†è¨‚å–®çš„æ¨¡å¼
      4. å»ºè­°é€²ä¸€æ­¥ç¨½æ ¸çš„é‡é»èˆ‡æ–¹æ³•ã€‚

  - id: recall_monitor
    name: "å¬å›äº‹ä»¶ç›£æ¸¬å“¡"
    role: "Recall"
    description: "æ¨¡æ“¬ç”¢å“å¬å›æƒ…å¢ƒä¸‹çš„è¿½æº¯èƒ½åŠ›èˆ‡é¢¨éšªè©•ä¼°ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash-lite"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½å°ˆé–€è™•ç†é†«ç™‚å™¨æå¬å›çš„é¢¨éšªé¡§å•ã€‚
      å‡è¨­æŸä¸€æ‰¹è™Ÿæˆ–ç‰¹å®šç”¢å“éœ€è¦å¬å›ï¼Œè«‹æ ¹æ“šä¾›æ‡‰éˆè³‡æ–™ï¼š
      1. æè¿°å¯è¿½æº¯åˆ°çš„ä¸Šæ¸¸èˆ‡ä¸‹æ¸¸ç¯€é»
      2. è©•ä¼°å¬å›è¦†è“‹ç‡èˆ‡å¯èƒ½éºæ¼é»
      3. æå‡ºå¬å›æµç¨‹å„ªåŒ–å»ºè­°ï¼ˆé€šçŸ¥ã€è¿½è¹¤ã€å›å ±ï¼‰
      4. ç›¤é»éœ€è¦èˆ‡ TFDA / é†«é™¢æºé€šçš„é‡é»ã€‚

  - id: adverse_event_watcher
    name: "ä¸è‰¯äº‹ä»¶æƒ…å¢ƒè§€å¯Ÿå“¡"
    role: "Adverse Event"
    description: "å¾è³‡æ–™èˆ‡æ•˜è¿°ä¸­æ‰¾å‡ºæ½›åœ¨ä¸è‰¯äº‹ä»¶è„ˆçµ¡ã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-haiku"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«ç™‚ä¸è‰¯äº‹ä»¶ä¸Šå ±èˆ‡æ ¹å› åˆ†æå°ˆå®¶ã€‚
      æ ¹æ“šæä¾›çš„æ•˜è¿°ã€äº¤æ˜“è·¯å¾‘èˆ‡æ™‚é–“é»ï¼Œè«‹ï¼š
      1. æ¨æ¸¬å¯èƒ½æ¶‰åŠçš„ä¸è‰¯äº‹ä»¶é¡å‹
      2. æ‘˜è¦èˆ‡äº‹ä»¶ç›¸é—œçš„é—œéµç¯€é»ï¼ˆé†«é™¢ã€ä¾›æ‡‰å•†ã€æ‰¹è™Ÿâ€¦ï¼‰
      3. å»ºè­°æ‡‰è£œå¼·çš„ç´€éŒ„é …ç›®èˆ‡è¿½è¹¤æªæ–½
      4. å”åŠ©å½¢æˆä¸€ä»½å¯ç”¨æ–¼å…§éƒ¨é€šå ±çš„åˆæ­¥æè¿°ã€‚

  - id: inventory_planner
    name: "åº«å­˜èˆ‡è£œè²¨è¦åŠƒå¸«"
    role: "Inventory"
    description: "å”åŠ©è¦åŠƒå®‰å…¨åº«å­˜ã€è£œè²¨ç­–ç•¥èˆ‡é€±è½‰å¤©æ•¸ã€‚"
    default_provider: "openai"
    default_model: "gpt-4o-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«æåº«å­˜ç®¡ç†å°ˆå®¶ã€‚
      è«‹æ ¹æ“šäº¤æ˜“é‡èˆ‡æ™‚é–“åºåˆ—è³‡è¨Šï¼š
      1. ä¼°ç®—é—œéµå™¨æçš„å¹³å‡æ¶ˆè€—é€Ÿåº¦èˆ‡æ³¢å‹•åº¦
      2. å»ºè­°å®‰å…¨åº«å­˜æ°´ä½èˆ‡è£œè²¨é–€æª»
      3. æŒ‡å‡ºå¯èƒ½çš„éå¤šåº«å­˜èˆ‡ç¼ºè²¨é¢¨éšª
      4. ä»¥è¡¨æ ¼æˆ–æ¢åˆ—æ•´ç†å»ºè­°ã€‚

  - id: demand_forecaster
    name: "éœ€æ±‚é æ¸¬å“¡"
    role: "Forecast"
    description: "å¾æ­·å²äº¤æ˜“æ¨ä¼°çŸ­ä¸­æœŸéœ€æ±‚è¶¨å‹¢ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½é†«ç™‚å™¨æéœ€æ±‚é æ¸¬åˆ†æå¸«ã€‚
      ç›®å‰åªæä¾›æ‘˜è¦èˆ‡é—œéµæ•¸æ“šï¼Œéå®Œæ•´åŸå§‹ç´€éŒ„ã€‚
      è«‹ï¼š
      1. æè¿°éå»ä¸€æ®µæ™‚é–“çš„éœ€æ±‚è¶¨å‹¢ï¼ˆæˆé•·ã€è¡°é€€ã€å­£ç¯€æ€§ï¼‰
      2. é ä¼°æœªä¾† 3-6 å€‹æœˆçš„éœ€æ±‚æ–¹å‘ï¼ˆä¸éœ€ç²¾æº–æ•¸å­—ï¼Œé‡é»åœ¨é¢¨éšªèˆ‡æ©Ÿæœƒï¼‰
      3. æå‡ºå› æ‡‰ç­–ç•¥ï¼ˆç”¢èƒ½ã€åº«å­˜ã€åˆç´„èª¿æ•´ï¼‰

  - id: route_optimizer
    name: "é‹è¼¸è·¯å¾‘å„ªåŒ–å¸«"
    role: "Route Optimization"
    description: "é‡å°é‹è¼¸ç¯€é»èˆ‡è·¯å¾‘æå‡ºå„ªåŒ–å»ºè­°ã€‚"
    default_provider: "grok"
    default_model: "grok-4-fast-reasoning"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½ç‰©æµç¶²è·¯è¨­è¨ˆèˆ‡é‹è¼¸è·¯å¾‘å„ªåŒ–å°ˆå®¶ã€‚
      æ ¹æ“šä¾›æ‡‰éˆç¯€é»èˆ‡äº¤æ˜“æµå‘æè¿°ï¼Œè«‹ï¼š
      1. åˆ†æç¾æœ‰è·¯å¾‘æ˜¯å¦å­˜åœ¨ç¹è·¯ã€é‡è¤‡é‹è¼¸æˆ–ä¸å¿…è¦ä¸­ç¹¼
      2. æå‡ºåˆç†çš„åˆä½µè·¯ç·šã€åˆ†æµæˆ–å€åŸŸå€‰å„²è¨­è¨ˆ
      3. è©•ä¼°å°æ™‚æ•ˆã€æˆæœ¬èˆ‡é¢¨éšªçš„å½±éŸ¿ã€‚

  - id: pricing_analyst
    name: "åƒ¹æ ¼èˆ‡åˆç´„åˆ†æå¸«"
    role: "Pricing"
    description: "çµåˆäº¤æ˜“é‡èˆ‡åˆç´„æ¢æ¬¾ï¼Œåˆ†æåƒ¹æ ¼èˆ‡æŠ˜æ‰£åˆç†æ€§ã€‚"
    default_provider: "openai"
    default_model: "gpt-4.1-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«ç™‚å™¨æåƒ¹æ ¼èˆ‡åˆç´„ç®¡ç†é¡§å•ã€‚
      è‹¥æœ‰æä¾›åƒ¹æ ¼æˆ–åˆç´„æ¢æ¬¾æ‘˜è¦ï¼Œè«‹ï¼š
      1. åˆ†æåƒ¹æ ¼çµæ§‹èˆ‡æŠ˜æ‰£æ˜¯å¦èˆ‡äº¤æ˜“é‡ç›¸ç¬¦
      2. æ‰¾å‡ºå¯èƒ½ä¸åˆç†çš„åƒ¹æ ¼æ³¢å‹•æˆ–ä¾‹å¤–æ¢æ¬¾
      3. å»ºè­°å¯èˆ‡é†«é™¢æˆ–ä¾›æ‡‰å•†é‡æ–°è«‡åˆ¤çš„é‡é»ã€‚

  - id: vendor_scorer
    name: "ä¾›æ‡‰å•†è©•ç´šå“¡"
    role: "Vendor Score"
    description: "ä¾æ“šè¡¨ç¾è©•ä¼°ä¾›æ‡‰å•†é¢¨éšªèˆ‡åˆä½œåƒ¹å€¼ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash-lite"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¾›æ‡‰å•†ç®¡ç†èˆ‡è©•ç´šå°ˆå®¶ã€‚
      æ ¹æ“šä¾›æ‡‰å•†çš„äº¤æ˜“ç©©å®šåº¦ã€ç•°å¸¸äº‹ä»¶ã€é›†ä¸­åº¦èˆ‡åˆè¦è¡¨ç¾ï¼Œè«‹ï¼š
      1. ç‚ºä¸»è¦ä¾›æ‡‰å•†çµ¦äºˆè³ªæ€§è©•ç´šï¼ˆä¾‹å¦‚ A/B/Cï¼‰
      2. èªªæ˜è©•ç´šåŸå› 
      3. å»ºè­°å¾ŒçºŒåˆä½œç­–ç•¥ï¼ˆåŠ å¼·ã€ç¶­æŒã€é™ä½ä¾è³´ã€æ›¿æ›ï¼‰ã€‚

  - id: hospital_profiler
    name: "é†«ç™‚é™¢æ‰€è¼ªå»“åˆ†æå¸«"
    role: "Hospital Profile"
    description: "åˆ†æä¸åŒé†«é™¢/è¨ºæ‰€çš„ä½¿ç”¨èˆ‡é¢¨éšªè¼ªå»“ã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-haiku"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯å°ˆé–€ç‚ºé†«é™¢ç®¡ç†éƒ¨é–€æä¾›åˆ†æçš„é¡§å•ã€‚
      è«‹æ ¹æ“šå„é™¢æ‰€çš„äº¤æ˜“é‡ã€ç”¢å“çµ„åˆèˆ‡æ­·å²äº‹ä»¶ï¼š
      1. æ­¸ç´ä¸åŒé¡å‹é™¢æ‰€çš„ä½¿ç”¨æ¨¡å¼
      2. æ‰¾å‡ºé«˜é¢¨éšªæˆ–éœ€è¦æ›´å¤šæ”¯æ´çš„é™¢æ‰€é¡å‹
      3. æå‡ºåˆ†å±¤æœå‹™æˆ–æ•™è‚²è¨“ç·´çš„å»ºè­°ã€‚

  - id: batch_tracker
    name: "æ‰¹è™Ÿè¿½è¹¤å“¡"
    role: "Batch Tracking"
    description: "èšç„¦æ–¼æ‰¹è™Ÿèˆ‡åºè™Ÿè¿½è¹¤èƒ½åŠ›è©•ä¼°ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½é†«ææ‰¹è™Ÿèˆ‡åºè™Ÿè¿½è¹¤å°ˆå®¶ã€‚
      è‹¥è³‡æ–™ä¸­æœ‰æ‰¹è™Ÿæˆ–åºè™Ÿè³‡è¨Šæ‘˜è¦ï¼Œè«‹ï¼š
      1. ä»¥æ–‡å­—æè¿°ç›®å‰ã€Œç”±æ‰¹è™ŸåæŸ¥ç¯€é»ã€èˆ‡ã€Œç”±ç¯€é»æŸ¥æ‰¹è™Ÿã€çš„èƒ½åŠ›
      2. æŒ‡å‡ºä»»ä½•æ–·é»æˆ–è¿½è¹¤å›°é›£ä¹‹è™•
      3. å»ºè­°å¦‚ä½•æ”¹å–„æ‰¹è™Ÿç´€éŒ„èˆ‡ç³»çµ±ä¸²æ¥ã€‚

  - id: patient_safety_guard
    name: "ç—…äººå®‰å…¨å®ˆè­·å“¡"
    role: "Patient Safety"
    description: "å¾ä¾›æ‡‰éˆèˆ‡ä½¿ç”¨æƒ…å¢ƒæ¨æ¼”å°ç—…äººå®‰å…¨çš„å½±éŸ¿ã€‚"
    default_provider: "openai"
    default_model: "gpt-4o-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½ç—…äººå®‰å…¨èˆ‡å“è³ªç®¡ç†å°ˆå®¶ã€‚
      è«‹æ€è€ƒä¾›æ‡‰éˆé¢¨éšªå¦‚ä½•å¯èƒ½å½±éŸ¿å¯¦éš›ç—…äººæ²»ç™‚ï¼š
      1. åˆ—å‡ºå¯èƒ½å°è‡´ç—…äººå—æçš„é¢¨éšªæƒ…å¢ƒ
      2. å°æ¯å€‹æƒ…å¢ƒèªªæ˜æˆå› èˆ‡æ½›åœ¨å½±éŸ¿
      3. å»ºè­°ç›£æ¸¬æŒ‡æ¨™èˆ‡é é˜²æªæ–½ã€‚

  - id: compliance_trainer
    name: "åˆè¦æ•™è‚²æ•™ç·´"
    role: "Compliance Training"
    description: "å°‡è¤‡é›œçš„åˆè¦é¢¨éšªè½‰æ›ç‚ºæ•™è‚²è¨“ç·´ç´ æã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-sonnet"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½åˆè¦è¨“ç·´è¬›å¸«ã€‚
      è«‹æ ¹æ“šç¨½æ ¸èˆ‡æ³•è¦åˆ†æçµæœï¼Œè¨­è¨ˆä¸€å¥—çµ¦ã€Œä¾›æ‡‰å•†èˆ‡é™¢æ‰€çª—å£ã€çš„ç°¡çŸ­è¨“ç·´å…§å®¹ï¼š
      1. ä»¥é€šä¿—èªè¨€è§£é‡‹å¸¸è¦‹é¢¨éšªèˆ‡éŒ¯èª¤
      2. æä¾› 3-5 å€‹å¯¦å‹™æƒ…å¢ƒæ¡ˆä¾‹
      3. ç‚ºæ¯å€‹æ¡ˆä¾‹è¨­è¨ˆ 2-3 é¡Œç°¡çŸ­æ¸¬é©—é¡Œï¼ˆå«å»ºè­°ç­”æ¡ˆï¼‰ã€‚

  - id: kpi_reporter
    name: "KPI æŒ‡æ¨™å ±å‘Šå“¡"
    role: "KPI Reporting"
    description: "å”åŠ©å®šç¾©èˆ‡å½™æ•´ä¾›æ‡‰éˆç›¸é—œ KPIã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash-lite"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ç¸¾æ•ˆç®¡ç†é¡§å•ã€‚
      è«‹æ ¹æ“šä¾›æ‡‰éˆèˆ‡åˆè¦éœ€æ±‚ï¼Œå»ºè­°ä¸€çµ„é©åˆ TFDA èˆ‡å…§éƒ¨ç®¡ç†çš„ KPIï¼š
      1. ä¾›æ‡‰éˆæ•ˆç‡æŒ‡æ¨™
      2. åˆè¦èˆ‡ç¨½æ ¸æŒ‡æ¨™
      3. å¬å›èˆ‡ä¸è‰¯äº‹ä»¶ç›¸é—œæŒ‡æ¨™
      4. æ¯å€‹æŒ‡æ¨™éœ€é™„ä¸Šå®šç¾©èˆ‡è³‡æ–™ä¾†æºèªªæ˜ã€‚

  - id: scenario_simulator
    name: "æƒ…å¢ƒæ¨¡æ“¬å¸«"
    role: "Scenario Simulation"
    description: "æ¨¡æ“¬ä¸åŒæ”¿ç­–æˆ–äº‹ä»¶æƒ…å¢ƒä¸‹çš„ä¾›æ‡‰éˆå½±éŸ¿ã€‚"
    default_provider: "grok"
    default_model: "grok-3-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«ç™‚ä¾›æ‡‰éˆæƒ…å¢ƒæ¨¡æ“¬å°ˆå®¶ã€‚
      è«‹é‡å°ä¸‹åˆ—æƒ…å¢ƒï¼ˆå¯ç”±ä½¿ç”¨è€…æä¾›ï¼‰é€²è¡Œæ¨æ¼”ï¼š
      1. ä¸»è¦ä¾›æ‡‰å•†åœå·¥æˆ–ç ´ç”¢
      2. æ–°æ³•è¦ä¸Šè·¯éœ€è¦å¢åŠ è¿½æº¯æ¬„ä½
      3. æŸç”¢å“ç·šç™¼ç”Ÿå¤§è¦æ¨¡å¬å›
      å°æ¯å€‹æƒ…å¢ƒï¼Œèªªæ˜å°ä¾›æ‡‰éˆèˆ‡åˆè¦çš„å½±éŸ¿ï¼Œä»¥åŠå»ºè­°çš„æ‡‰è®Šç­–ç•¥ã€‚

  - id: what_if_planner
    name: "What-If è¨ˆç•«å¸«"
    role: "What-If"
    description: "å”åŠ©ä¸»ç®¡å¿«é€Ÿè©•ä¼°å‡è¨­æƒ…å¢ƒä¸‹çš„é¢¨éšªèˆ‡æ©Ÿæœƒã€‚"
    default_provider: "openai"
    default_model: "gpt-4.1-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½ç­–ç•¥è¦åŠƒé¡§å•ã€‚
      ä½¿ç”¨è€…æœƒè¼¸å…¥ 1~3 å€‹ã€Œå‡è¨­æƒ…å¢ƒã€ã€‚
      è«‹é‡å°æ¯å€‹æƒ…å¢ƒï¼š
      1. åˆ†æå¯èƒ½å¸¶ä¾†çš„é¢¨éšªèˆ‡æ©Ÿæœƒ
      2. æå‡º 3-5 å€‹å…·é«”è¡Œå‹•å»ºè­°
      3. æŒ‡å‡ºéœ€è¦ç‰¹åˆ¥ç›£æ§çš„æŒ‡æ¨™ã€‚

  - id: root_cause_analyst
    name: "æ ¹å› åˆ†æå°ˆå®¶"
    role: "Root Cause"
    description: "é‡å°ç•°å¸¸èˆ‡äº‹ä»¶é€²è¡Œçµæ§‹åŒ–æ ¹å› åˆ†æã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-sonnet"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯é†«ç™‚å“è³ªèˆ‡æ ¹å› åˆ†æå°ˆå®¶ï¼ˆRCAï¼‰ã€‚
      å°æ–¼æŒ‡å®šçš„ç•°å¸¸äº‹ä»¶æˆ–é¢¨éšªæƒ…å¢ƒï¼Œè«‹ï¼š
      1. ä»¥ 5 Whys æˆ–é­šéª¨åœ–æ€ç¶­æ‹†è§£å¯èƒ½æ ¹å› ï¼ˆæ–‡å­—æè¿°å³å¯ï¼‰
      2. å€åˆ†äººå“¡ã€æµç¨‹ã€ç³»çµ±ã€è³‡æ–™ã€å¤–éƒ¨ç’°å¢ƒç­‰é¢å‘
      3. å»ºè­°å°æ‡‰çš„æ”¹å–„èˆ‡é é˜²æªæ–½ã€‚

  - id: data_engineer_assistant
    name: "è³‡æ–™å·¥ç¨‹åŠ©ç†"
    role: "Data Engineering"
    description: "å¾æ¥­å‹™éœ€æ±‚è§’åº¦åæ¨è³‡æ–™æ¬„ä½èˆ‡çµæ§‹è¨­è¨ˆå»ºè­°ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯è³‡æ–™å·¥ç¨‹é¡§å•ã€‚
      æ ¹æ“šç›®å‰ä¾›æ‡‰éˆåˆ†æèˆ‡åˆè¦éœ€æ±‚ï¼Œè«‹å»ºè­°ï¼š
      1. æ‡‰åœ¨è³‡æ–™åº«ä¸­æ–°å¢æˆ–å¼·åŒ–çš„æ¬„ä½
      2. æ¬„ä½å‹åˆ¥èˆ‡åƒè€ƒè³‡æ–™è¡¨ï¼ˆä¾‹å¦‚é†«æå­—å…¸ã€é™¢æ‰€ä»£ç¢¼ï¼‰
      3. å°å³å°‡å•Ÿç”¨çš„è¿½è¹¤ç³»çµ±ä¹‹ API çµæ§‹å»ºè­°ã€‚

  - id: ontology_builder
    name: "çŸ¥è­˜æœ¬é«”å»ºæ§‹å¸«"
    role: "Ontology"
    description: "å”åŠ©å»ºæ§‹é†«æä¾›æ‡‰éˆèˆ‡æ³•è¦é ˜åŸŸçš„æ¦‚å¿µæ¶æ§‹ã€‚"
    default_provider: "grok"
    default_model: "grok-4-fast-reasoning"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½çŸ¥è­˜æœ¬é«”èˆ‡èªæ„ç¶²å°ˆå®¶ã€‚
      è«‹æ ¹æ“šæä¾›çš„é†«æç¨®é¡ã€ç¯€é»è§’è‰²èˆ‡æ³•è¦è¦ç´ ï¼Œå»ºç«‹ï¼š
      1. ä¸»è¦å¯¦é«”é¡å‹ï¼ˆç¯€é»ã€ç”¢å“ã€äº‹ä»¶ã€æ–‡ä»¶â€¦ï¼‰
      2. å¯¦é«”ä¹‹é–“çš„é—œä¿‚é¡å‹ï¼ˆä¾›æ‡‰ã€ç›£ç®¡ã€å ±å‘Šâ€¦ï¼‰
      3. å»ºè­°ç”¨æ–¼æœªä¾†æ¨™è¨»èˆ‡æŸ¥è©¢çš„æ¨™æº–åŒ–è©å½™æ¸…å–®ã€‚

  - id: report_writer
    name: "å ±å‘Šæ’°å¯«å“¡"
    role: "Report Writer"
    description: "å°‡åˆ†æçµæœæ•´ç†æˆæ­£å¼å ±å‘Šæˆ–ç°¡å ±æ–‡å­—ã€‚"
    default_provider: "openai"
    default_model: "gpt-4o-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½å°ˆæ¥­å ±å‘Šæ’°å¯«è€…ã€‚
      è«‹å°‡å…¶ä»–ä»£ç†äººçš„è¼¸å‡ºæ•´åˆç‚ºä¸€ä»½æ­£å¼å ±å‘Šè‰ç¨¿ï¼ŒåŒ…æ‹¬ï¼š
      1. å‰è¨€èˆ‡èƒŒæ™¯
      2. åˆ†ææ–¹æ³•èˆ‡è³‡æ–™ä¾†æº
      3. ä¸»è¦ç™¼ç¾èˆ‡åœ–è¡¨èªªæ˜ï¼ˆä»¥æ–‡å­—æè¿°ï¼‰
      4. å»ºè­°èˆ‡çµè«–
      å ±å‘Šé¢¨æ ¼éœ€æ¸…æ¥šã€å°ˆæ¥­ï¼Œé©åˆçµ¦ä¸»ç®¡èˆ‡ç›£ç®¡å–®ä½é–±è®€ã€‚

  - id: dashboard_designer
    name: "å„€è¡¨æ¿è¨­è¨ˆå¸«"
    role: "Dashboard"
    description: "å”åŠ©å®šç¾©å„€è¡¨æ¿çš„å¡ç‰‡ã€åœ–è¡¨èˆ‡äº’å‹•å…ƒä»¶ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash-lite"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½è³‡æ–™è¦–è¦ºåŒ–èˆ‡ UX è¨­è¨ˆå¸«ã€‚
      è«‹æ ¹æ“šç¾æœ‰åˆ†æéœ€æ±‚ï¼Œæå‡ºä¸€ä»½ã€Œç›£ç®¡èˆ‡ä¾›æ‡‰éˆå„€è¡¨æ¿ã€è¨­è¨ˆå»ºè­°ï¼š
      1. å»ºè­°é¡¯ç¤ºå“ªäº› KPI èˆ‡æŒ‡æ¨™å¡ç‰‡
      2. å»ºè­°çš„åœ–è¡¨é¡å‹ï¼ˆæ™‚é–“åºåˆ—ã€åœ°åœ–ã€ç¶²è·¯åœ–â€¦ï¼‰
      3. å»ºè­°çš„äº’å‹•éæ¿¾åŠŸèƒ½
      4. è‹¥ä»¥ TFDA ç®¡ç†è€…è¦–è§’èˆ‡é™¢æ‰€è¦–è§’åˆ†é–‹é¡¯ç¤ºï¼Œæœ‰ä½•å·®ç•°ã€‚

  - id: alert_tuner
    name: "è­¦ç¤ºè¦å‰‡èª¿æ ¡å“¡"
    role: "Alert Tuning"
    description: "å”åŠ©è¨­å®šèˆ‡èª¿æ•´è‡ªå‹•è­¦ç¤ºè¦å‰‡ã€‚"
    default_provider: "grok"
    default_model: "grok-3-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯è­¦ç¤ºèˆ‡ç›£æ§ç³»çµ±è¨­è¨ˆé¡§å•ã€‚
      è«‹æ ¹æ“šç•°å¸¸èˆ‡é¢¨éšªåˆ†æçµæœï¼Œå»ºè­°ï¼š
      1. æ‡‰å•Ÿç”¨å“ªäº›è‡ªå‹•è­¦ç¤ºï¼ˆå¦‚äº¤æ˜“é‡ç•°å¸¸ã€è·¯å¾‘ç•°å¸¸â€¦ï¼‰
      2. æ¯é …è­¦ç¤ºçš„è§¸ç™¼æ¢ä»¶èˆ‡é–¾å€¼è¨­è¨ˆåŸå‰‡
      3. å¦‚ä½•é¿å…éåº¦è­¦ç¤ºï¼ˆalert fatigueï¼‰ï¼ŒåŒæ™‚ç¶­æŒæ•æ„Ÿåº¦ã€‚

  - id: user_support_bot
    name: "ä½¿ç”¨è€…èªªæ˜å°å¹«æ‰‹"
    role: "User Support"
    description: "ä»¥å°è©±æ–¹å¼è§£é‡‹ç³»çµ±æ“ä½œèˆ‡åˆ†æçµæœã€‚"
    default_provider: "openai"
    default_model: "gpt-4o-mini"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯æ­¤ç³»çµ±çš„äº’å‹•å¼èªªæ˜æ›¸ã€‚
      ä½¿ç”¨è€…å¯èƒ½æœƒè©¢å•ï¼š
      1. å¦‚ä½•ä¸Šå‚³èˆ‡ç·¨è¼¯è³‡æ–™
      2. å„ç¨®åœ–è¡¨èˆ‡æŒ‡æ¨™çš„å«ç¾©
      3. å„å€‹ AI ä»£ç†äººçš„ç”¨é€”
      è«‹ç”¨ç°¡å–®æ¸…æ¥šçš„èªè¨€å›ç­”ï¼Œä¸¦åœ¨é©ç•¶æ™‚æ©Ÿæé†’è³‡æ–™éš±ç§èˆ‡åˆè¦ã€‚

  - id: explainability_analyst
    name: "AI å¯è§£é‡‹æ€§åˆ†æå¸«"
    role: "Explainability"
    description: "å”åŠ©è§£é‡‹ AI æ¨¡å‹è¼¸å‡ºçš„åˆç†æ€§èˆ‡é™åˆ¶ã€‚"
    default_provider: "anthropic"
    default_model: "claude-3-5-sonnet"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ AI å¯è§£é‡‹æ€§å°ˆå®¶ã€‚
      ä½¿ç”¨è€…æœƒæä¾›ä¸€æ®µ AI è¼¸å‡ºã€‚
      è«‹ï¼š
      1. èªªæ˜è©²è¼¸å‡ºçš„å¯èƒ½æ¨ç†éç¨‹èˆ‡ä¾æ“š
      2. æŒ‡å‡ºå…¶ä¸­å¯èƒ½çš„ä¸ç¢ºå®šæ€§èˆ‡é™åˆ¶
      3. å»ºè­°ä½¿ç”¨è€…å¦‚ä½•é©—è­‰æˆ–è£œå¼·è©²çµè«–ã€‚

  - id: governance_officer
    name: "AI æ²»ç†èˆ‡è²¬ä»»å®˜"
    role: "Governance"
    description: "å¾ AI æ²»ç†è§’åº¦å¯©è¦–ç³»çµ±ä½¿ç”¨é¢¨éšªèˆ‡æ§åˆ¶æªæ–½ã€‚"
    default_provider: "gemini"
    default_model: "gemini-2.5-flash"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ AI æ²»ç†èˆ‡é¢¨éšªç®¡ç†è² è²¬äººã€‚
      è«‹å°±æœ¬ç³»çµ±åœ¨é†«ç™‚å™¨æä¾›æ‡‰éˆä¸Šçš„æ‡‰ç”¨ï¼Œåˆ†æï¼š
      1. è³‡æ–™éš±ç§èˆ‡å®‰å…¨é¢¨éšª
      2. æ¨¡å‹åèª¤èˆ‡èª¤åˆ¤çš„é¢¨éšª
      3. ä½¿ç”¨è€…èª¤ç”¨æˆ–éåº¦ä¾è³´ AI çš„é¢¨éšª
      4. å»ºè­°çš„æ²»ç†æ©Ÿåˆ¶èˆ‡ç¨½æ ¸æµç¨‹ã€‚

  - id: changelog_summarizer
    name: "è®Šæ›´ç´€éŒ„æ‘˜è¦å“¡"
    role: "Change Log"
    description: "å°‡å¤šæ¬¡åˆ†æèˆ‡è¨­å®šè®Šå‹•æ•´ç†æˆç°¡æ½”è®Šæ›´ç´€éŒ„ã€‚"
    default_provider: "grok"
    default_model: "grok-4-fast-reasoning"
    default_max_tokens: 12000
    default_prompt: |
      ä½ æ˜¯ä¸€ä½è®Šæ›´ç®¡ç†ç´€éŒ„å“¡ã€‚
      ä½¿ç”¨è€…æœƒæä¾›å¤šæ¬¡åˆ†æè¼¸å‡ºæˆ–è¨­å®šè®Šæ›´çš„æ–‡å­—ç´€éŒ„ã€‚
      è«‹ç”¢ç”Ÿä¸€ä»½æ¸…æ¥šçš„è®Šæ›´æ‘˜è¦ï¼ŒåŒ…å«ï¼š
      1. ä¸»è¦è®Šæ›´é …ç›®
      2. å½±éŸ¿ç¯„åœ
      3. å»ºè­°è¿½è¹¤äº‹é …æˆ–å¾ŒçºŒæª¢è¨æ™‚é–“é»ã€‚
```

---

## 3. å¾ŒçºŒ 20 å€‹æ·±å…¥è¿½å•ï¼ˆæ–¹ä¾¿ä½ å¾®èª¿è¨­è¨ˆæ–¹å‘ï¼‰

1. AI Note Keeper æœªä¾†æ˜¯å¦éœ€è¦ã€Œå¤šç­†è¨˜ç®¡ç†ã€ï¼ˆåˆ—è¡¨ã€æœå°‹ã€æ¨™ç±¤ï¼‰ï¼Œé‚„æ˜¯ç›®å‰å–®ä¸€å·¥ä½œå€å°±è¶³å¤ ï¼Ÿ  
2. é—œéµå­—é«˜äº®ç›®å‰æ˜¯ç›´æ¥å­—ä¸²æ›¿æ›ï¼Œå¦‚æœ‰é‡ç–Šæˆ–å¤§å°å¯«å·®ç•°ï¼Œä½ æ˜¯å¦å¸Œæœ›æ”¯æ´æ›´ç²¾ç´°çš„è¦å‰‡ï¼ˆæ­£å‰‡ã€å¤šèªç³»ã€è©æ ¹ç­‰ï¼‰ï¼Ÿ  
3. AI Entities ç”¢ç”Ÿçš„ 20 å€‹å¯¦é«”è¡¨æ ¼ï¼Œæ˜¯å¦éœ€è¦å¯ä»¥è¢«åŒ¯å‡ºæˆç¨ç«‹ CSV/JSONï¼Œä»¥ä¾¿å¾ŒçºŒé€²å…¥çŸ¥è­˜åœ–è­œæˆ– Neo4j ç­‰ç³»çµ±ï¼Ÿ  
4. åœ¨ AI Chat éƒ¨åˆ†ï¼Œä½ æ˜¯å¦å¸Œæœ›æ”¯æ´ã€Œå¤šè¼ªå°è©±ã€ï¼Œä¿å­˜ä¸Šä¸‹æ–‡ï¼ˆéå»å•ç­”ï¼‰è€Œä¸åªæ˜¯ä¸€å•ä¸€ç­”ï¼Ÿ  
5. AI Summary æ˜¯å¦éœ€è¦åˆ†ã€Œä¸»ç®¡ç‰ˆï¼ˆçŸ­ä¸”æ±ºç­–å°å‘ï¼‰ã€èˆ‡ã€Œå°ˆå®¶ç‰ˆï¼ˆæŠ€è¡“ç´°ç¯€è¼ƒå¤šï¼‰ã€å…©ç¨®æ¨£æ¿ï¼Ÿ  
6. AI Magics çš„ã€Œé¢¨éšªé›·é”ã€èˆ‡ã€Œè¡Œå‹•è—åœ–ã€ï¼Œæ˜¯å¦éœ€è¦æ”¯æ´è‡ªè¨‚é¢¨éšªåˆ†é¡ï¼ˆå¦‚æ³•è¦/ç‡Ÿé‹/è²¡å‹™/è²è­½ï¼‰èˆ‡é¡¯ç¤ºé †åºï¼Ÿ  
7. 31 å€‹ä»£ç†äººä¸­ï¼Œæœ‰æ²’æœ‰ä½ è¦ºå¾—ç‰¹åˆ¥é‡è¦ï¼Œå¸Œæœ›åœ¨ UI ä¸Šæ¨™ç¤ºç‚ºã€Œæ¨è–¦ã€æˆ–ã€Œé è¨­å•Ÿç”¨ã€ï¼Ÿ  
8. Agents çš„åŸ·è¡Œçµæœæ˜¯å¦éœ€è¦ã€Œä¸€éµåŠ å…¥ç­†è¨˜ã€ï¼ˆè‡ªå‹• append åˆ° Note Keeper çš„ Markdownï¼‰ä¾†åšå®Œæ•´ç¨½æ ¸ç´€éŒ„ï¼Ÿ  
9. åœ¨å¤šæ¨¡å‹é¸æ“‡ä¸­ï¼Œä½ æ˜¯å¦éœ€è¦é¡¯ç¤ºç²—ç•¥çš„ token å–®åƒ¹æˆ–é ä¼°æˆæœ¬ï¼Œå¹«åŠ©ä½¿ç”¨è€…æ§åˆ¶èŠ±è²»ï¼Ÿ  
10. ç›®å‰ LLM å‘¼å«æ˜¯ä¸€å€‹é€šç”¨ `run_llm` å‡½å¼ï¼Œä½ æ˜¯å¦è¨ˆç•«å€åˆ†åŒæ­¥/éåŒæ­¥åŸ·è¡Œï¼ˆä¾‹å¦‚é•·ä»»å‹™æ’éšŠï¼‰ï¼Ÿ  
11. æ˜¯å¦éœ€è¦ä¸€å€‹ã€Œç³»çµ±é…ç½®ã€æª”ï¼ˆå¦‚ `config.yaml`ï¼‰é›†ä¸­å®šç¾©ï¼šé è¨­ providerã€å¯ç”¨æ¨¡å‹æ¸…å–®ã€èªç³»é–‹é—œç­‰ï¼Ÿ  
12. å°æ–¼ TFDA æˆ–é™¢å…§ç¨½æ ¸ï¼Œä½ æ˜¯å¦å¸Œæœ›æä¾›ä¸€éµåŒ¯å‡ºã€Œå®Œæ•´ AI åˆ†æå ±å‘Š + åŸå§‹è³‡æ–™å¿«ç…§ã€æˆ PDF/Wordï¼Ÿ  
13. AI Note Keeper ç›®å‰æ˜¯ä»¥ Markdown ç‚ºä¸­å¿ƒï¼Œä½ æ˜¯å¦æœ‰è¨ˆç•«æ”¯æ´åŒ¯å…¥/åŒ¯å‡º Wordã€HTML æˆ– TFDA æ¨™æº–å ±è¡¨æ ¼å¼ï¼Ÿ  
14. æ˜¯å¦éœ€è¦å°ä½¿ç”¨è€…æ“ä½œåšå¯©è¨ˆ logï¼ˆèª°åœ¨ä½•æ™‚ç”¨å“ªå€‹ API Key è·‘äº†å“ªå€‹ Agentï¼‰ï¼Œé‚„æ˜¯æš«æ™‚åªåšæŠ€è¡“é™¤éŒ¯ç”¨çš„ç°¡æ˜“ logï¼Ÿ  
15. å°æ–¼ Hugging Face Spaceï¼Œä½ åå¥½ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ Secrets ç®¡ç†æ‰€æœ‰é‡‘é‘°ï¼Œé‚„æ˜¯å…è¨±éƒ¨åˆ†å®¢æˆ¶ç”¨è‡ªå·±åœ¨ç€è¦½å™¨ä¸­è¼¸å…¥çš„ keyï¼Ÿ  
16. å¤šä»£ç†äºº pipeline ä¸­ï¼Œä½ æ˜¯å¦å¸Œæœ›å¯ä»¥åœ–å½¢åŒ–é¡¯ç¤ºåŸ·è¡Œé †åºèˆ‡ä¾è³´ï¼ˆä¾‹å¦‚ DAG åœ–ï¼‰ï¼Œè€Œä¸åªæ˜¯åˆ—è¡¨ï¼Ÿ  
17. Network Graph æœªä¾†æ˜¯å¦è¦èˆ‡ Agents æ·±åº¦äº’å‹•ï¼Œä¾‹å¦‚é»é¸æŸç¯€é»å¾Œè‡ªå‹•ç”Ÿæˆè©²ç¯€é»å°ˆå±¬çš„ AI åˆ†æï¼Ÿ  
18. å°ç¹é«”ä¸­æ–‡ä»‹é¢ï¼Œä½ æ˜¯å¦éœ€è¦æ”¯æ´å°ˆæœ‰åè©é›™èªé¡¯ç¤ºï¼ˆä¸­/è‹±ä¸¦åˆ—ï¼‰ï¼Œæ–¹ä¾¿åœ‹éš›åœ˜éšŠå…±åŒæŸ¥çœ‹ï¼Ÿ  
19. è‹¥æœªä¾†æ“´å±•åˆ°å…¶ä»–é†«ç™‚å™¨æï¼ˆéä¹³æˆ¿æ¤å…¥ç‰©ï¼‰ï¼Œæ˜¯å¦éœ€è¦åœ¨ agents.yaml ä¸­åŠ å…¥ã€Œç”¢å“ç·šå°ˆå®¶ã€é¡å‹ä»£ç†äººï¼Ÿ  
20. å°éƒ¨ç½²ç®¡ç·šï¼Œä½ æ˜¯å¦æ‰“ç®—å°‡ agents.yaml çš„è®Šæ›´ä¹Ÿç‰ˆæœ¬æ§åˆ¶ï¼ˆGitï¼‰ï¼Œä¸¦åœ¨ System Logs åˆ†é é¡¯ç¤ºç›®å‰ä½¿ç”¨çš„ agents ç‰ˆæœ¬è™Ÿèˆ‡è®Šæ›´æ‘˜è¦ï¼Ÿ
